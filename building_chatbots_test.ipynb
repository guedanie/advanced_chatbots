{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "am AUX\n",
      "learning VERB\n",
      "how ADV\n",
      "to PART\n",
      "build VERB\n",
      "chatbots NOUN\n"
     ]
    }
   ],
   "source": [
    "# example 1\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(u\"I am learning how to build chatbots\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "am AUX\n",
      "going VERB\n",
      "to ADP\n",
      "London PROPN\n",
      "next ADJ\n",
      "week NOUN\n",
      "for ADP\n",
      "a DET\n",
      "meeting NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# example 2\n",
    "doc = nlp(u'I am going to London next week for a meeting.')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Google PROPN NNP compound Xxxxx True False\n",
      "release release NOUN NN poss xxxx True False\n",
      "' ' PUNCT `` punct ' False False\n",
      "Movie Movie PROPN NNP compound Xxxxx True False\n",
      "Mirror Mirror PROPN NNP poss Xxxxx True False\n",
      "' ' PUNCT '' case ' False False\n",
      "AI AI PROPN NNP compound XX True False\n",
      "experiment experiment NOUN NN compound xxxx True False\n",
      "taht taht NOUN NN nsubj xxxx True False\n",
      "matches match VERB VBZ ROOT xxxx True False\n",
      "your -PRON- DET PRP$ poss xxxx True True\n",
      "pose pose NOUN NN dobj xxxx True False\n",
      "from from ADP IN prep xxxx True True\n",
      "80,000 80,000 NUM CD nummod dd,ddd False False\n",
      "images image NOUN NNS pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "# example 3\n",
    "\n",
    "doc = nlp(u\"Google release 'Movie Mirror' AI experiment taht matches your pose from 80,000 images\")\n",
    "          \n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named-Entity Recognition\n",
    "\n",
    "Named-entity recognition (NER) is a process of finding and classifying named entities existing in the given text into pre-defined categories. \n",
    "\n",
    "spaCy comes with a very fast entity recognition model that is capable of identifying entity phrases from a given document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "Muntain View GPE\n",
      "Californina GPE\n",
      "109.65 billion US dollars MONEY\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "\n",
    "my_string = u\"Google has its headquarters in Muntain View, Californina having revenue amounted to 109.65 billion US dollars\"\n",
    "doc = nlp(my_string)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark Zuckerberg PERSON\n",
      "May 14, 1984 DATE\n",
      "New York GPE\n",
      "American NORP\n",
      "Facebook ORG\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "\n",
    "my_string = u\"Mark Zuckerberg was born May 14, 1984 in New York is an American technology entrepreneur and philanthropist best known for co-founding and leading Facebook as its chairman and CEO.\"\n",
    "\n",
    "doc = nlp(my_string)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependecy Pasing \n",
    "\n",
    "This feature gives you a parsed tree that explains the parent-child relationship between the words or phrases and independent of the order in which words occur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[from, flight, Book]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example 1\n",
    "\n",
    "doc = nlp(u'Book me a flight from Bangalore to Goa')\n",
    "blr, goa = doc[5], doc[7]\n",
    "list(blr.ancestors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output can tell us that user is looking to book the flight from Bangalore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[to, flight, Book]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(goa.ancestors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the use of dependency parsing in chatbots?\n",
    "\n",
    "Dependency parsing is one of the most important parts  when building chatbots. \n",
    "\n",
    "* It helps in finding relationships between words of grammatically correct sentences\n",
    "* It can be used for sentenced boundary detection\n",
    "* It is quite useful tofind out if the user is taking about more than one context simultaniously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun Chunchs\n",
    "\n",
    "\"base noun phrases\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Boston Dynamics, thousands, robot dogs]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## example 1:\n",
    "doc = nlp(u\"Boston Dynamics is gearing up to produce thousands of robot dogs\")\n",
    "list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though having noun chuncks from a given sentence helps a lot, spaCy provides other attributes that can be helpful too. Let's try to explore some of those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning learning nsubj cracks\n",
      "the code code dobj cracks\n",
      "the messenger RNAs RNAs pobj of\n",
      "protein-coding potential potential conj RNAs\n"
     ]
    }
   ],
   "source": [
    "## example 2\n",
    "\n",
    "doc = nlp(u\"Deep learning cracks the code of the messenger RNAs and protein-coding potential\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Similarity\n",
    "\n",
    "GloVe is an unserpervised learning algorithm for obtaining vector representations for words. GloVe algorithm uses aggregated global word-word co-occurenece statistics from a corpus to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How [ 0.6124835   1.4812975  -0.72636694  0.06934005  4.5061274 ]\n",
      "are [-3.683414   1.6739068  0.8058587 -1.5030262 -1.456842 ]\n",
      "you [-1.982389   0.1802355 -1.5821393 -0.9292287  3.5329905]\n",
      "doing [ 1.4059318  -3.2353983  -0.499872   -1.289648   -0.42563045]\n",
      "today [ 1.0131264 -3.218736  -2.291664  -1.326089  -0.6509166]\n",
      "? [ 1.5639621  -1.0553905  -1.8733073  -1.7224176  -0.41755062]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"How are you doing today?\")\n",
    "for token in doc:\n",
    "    print(token.text, token.vector[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
